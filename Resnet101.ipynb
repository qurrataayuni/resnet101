{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipykernel in c:\\users\\hp\\miniconda3\\envs\\tf\\lib\\site-packages (6.15.0)\n",
            "Requirement already satisfied: pyzmq>=17 in c:\\users\\hp\\miniconda3\\envs\\tf\\lib\\site-packages (from ipykernel) (25.0.2)\n",
            "Requirement already satisfied: nest-asyncio in c:\\users\\hp\\miniconda3\\envs\\tf\\lib\\site-packages (from ipykernel) (1.5.6)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in c:\\users\\hp\\miniconda3\\envs\\tf\\lib\\site-packages (from ipykernel) (5.9.0)\n",
            "Requirement already satisfied: tornado>=6.1 in c:\\users\\hp\\miniconda3\\envs\\tf\\lib\\site-packages (from ipykernel) (6.2)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\hp\\miniconda3\\envs\\tf\\lib\\site-packages (from ipykernel) (0.1.6)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\hp\\miniconda3\\envs\\tf\\lib\\site-packages (from ipykernel) (8.1.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\hp\\miniconda3\\envs\\tf\\lib\\site-packages (from ipykernel) (23.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in c:\\users\\hp\\miniconda3\\envs\\tf\\lib\\site-packages (from ipykernel) (1.5.1)\n",
            "Requirement already satisfied: psutil in c:\\users\\hp\\miniconda3\\envs\\tf\\lib\\site-packages (from ipykernel) (5.9.0)\n",
            "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\hp\\miniconda3\\envs\\tf\\lib\\site-packages (from ipykernel) (8.11.0)\n",
            "Requirement already satisfied: pickleshare in c:\\users\\hp\\miniconda3\\envs\\tf\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.7.5)\n",
            "Requirement already satisfied: stack-data in c:\\users\\hp\\miniconda3\\envs\\tf\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.6.2)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\hp\\miniconda3\\envs\\tf\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (3.0.38)\n",
            "Requirement already satisfied: jedi>=0.16 in c:\\users\\hp\\miniconda3\\envs\\tf\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.18.2)\n",
            "Requirement already satisfied: backcall in c:\\users\\hp\\miniconda3\\envs\\tf\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n",
            "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\hp\\miniconda3\\envs\\tf\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (2.14.0)\n",
            "Requirement already satisfied: decorator in c:\\users\\hp\\miniconda3\\envs\\tf\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\hp\\miniconda3\\envs\\tf\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\miniconda3\\envs\\tf\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel) (2.8.2)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\hp\\miniconda3\\envs\\tf\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel) (5.3.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\hp\\miniconda3\\envs\\tf\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.3)\n",
            "Requirement already satisfied: pywin32>=300 in c:\\users\\hp\\miniconda3\\envs\\tf\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-client>=6.1.12->ipykernel) (305.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\hp\\miniconda3\\envs\\tf\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-client>=6.1.12->ipykernel) (3.2.0)\n",
            "Requirement already satisfied: wcwidth in c:\\users\\hp\\miniconda3\\envs\\tf\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=7.23.1->ipykernel) (0.2.6)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\miniconda3\\envs\\tf\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.16.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\hp\\miniconda3\\envs\\tf\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.2.1)\n",
            "Requirement already satisfied: pure-eval in c:\\users\\hp\\miniconda3\\envs\\tf\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.2.2)\n",
            "Requirement already satisfied: executing>=1.2.0 in c:\\users\\hp\\miniconda3\\envs\\tf\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (1.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install ipykernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: protobuf==3.20.* in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (3.20.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "pip install protobuf==3.20.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy-stack in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (0.0.5)\n",
            "Requirement already satisfied: nose>=1.3.7 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from scipy-stack) (1.3.7)\n",
            "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from scipy-stack) (1.10.0)\n",
            "Requirement already satisfied: matplotlib>=2.0.2 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from scipy-stack) (3.5.2)\n",
            "Requirement already satisfied: jupyter>=1.0.0 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from scipy-stack) (1.0.0)\n",
            "Requirement already satisfied: sympy>=1.1.1 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from scipy-stack) (1.11.1)\n",
            "Requirement already satisfied: numpy>=1.13.1 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from scipy-stack) (1.23.0)\n",
            "Requirement already satisfied: pandas>=0.20.3 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from scipy-stack) (1.4.2)\n",
            "Requirement already satisfied: qtconsole in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from jupyter>=1.0.0->scipy-stack) (5.3.1)\n",
            "Requirement already satisfied: ipywidgets in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from jupyter>=1.0.0->scipy-stack) (7.7.1)\n",
            "Requirement already satisfied: notebook in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from jupyter>=1.0.0->scipy-stack) (6.4.12)\n",
            "Requirement already satisfied: nbconvert in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from jupyter>=1.0.0->scipy-stack) (6.5.0)\n",
            "Requirement already satisfied: ipykernel in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from jupyter>=1.0.0->scipy-stack) (6.15.0)\n",
            "Requirement already satisfied: jupyter-console in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from jupyter>=1.0.0->scipy-stack) (6.4.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from matplotlib>=2.0.2->scipy-stack) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from matplotlib>=2.0.2->scipy-stack) (2.8.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from matplotlib>=2.0.2->scipy-stack) (9.2.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from matplotlib>=2.0.2->scipy-stack) (4.25.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from matplotlib>=2.0.2->scipy-stack) (3.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from matplotlib>=2.0.2->scipy-stack) (21.3)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from matplotlib>=2.0.2->scipy-stack) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from pandas>=0.20.3->scipy-stack) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=2.0.2->scipy-stack) (1.16.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from sympy>=1.1.1->scipy-stack) (1.2.1)\n",
            "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from ipykernel->jupyter>=1.0.0->scipy-stack) (8.4.0)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from ipykernel->jupyter>=1.0.0->scipy-stack) (5.3.0)\n",
            "Requirement already satisfied: psutil in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from ipykernel->jupyter>=1.0.0->scipy-stack) (5.9.1)\n",
            "Requirement already satisfied: tornado>=6.1 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from ipykernel->jupyter>=1.0.0->scipy-stack) (6.1)\n",
            "Requirement already satisfied: nest-asyncio in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from ipykernel->jupyter>=1.0.0->scipy-stack) (1.5.5)\n",
            "Requirement already satisfied: debugpy>=1.0 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from ipykernel->jupyter>=1.0.0->scipy-stack) (1.6.0)\n",
            "Requirement already satisfied: pyzmq>=17 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from ipykernel->jupyter>=1.0.0->scipy-stack) (23.2.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from ipykernel->jupyter>=1.0.0->scipy-stack) (7.3.4)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from ipykernel->jupyter>=1.0.0->scipy-stack) (0.1.2)\n",
            "Requirement already satisfied: backcall in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->scipy-stack) (0.2.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->scipy-stack) (61.2.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->scipy-stack) (0.4.5)\n",
            "Requirement already satisfied: decorator in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->scipy-stack) (5.1.1)\n",
            "Requirement already satisfied: pickleshare in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->scipy-stack) (0.7.5)\n",
            "Requirement already satisfied: jedi>=0.16 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->scipy-stack) (0.17.2)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->scipy-stack) (3.0.30)\n",
            "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->scipy-stack) (2.12.0)\n",
            "Requirement already satisfied: stack-data in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->scipy-stack) (0.3.0)\n",
            "Requirement already satisfied: parso<0.8.0,>=0.7.0 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->scipy-stack) (0.7.0)\n",
            "Requirement already satisfied: jupyter-core>=4.9.2 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter>=1.0.0->scipy-stack) (4.10.0)\n",
            "Requirement already satisfied: entrypoints in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter>=1.0.0->scipy-stack) (0.4)\n",
            "Requirement already satisfied: pywin32>=1.0 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel->jupyter>=1.0.0->scipy-stack) (304)\n",
            "Requirement already satisfied: wcwidth in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->scipy-stack) (0.2.5)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from ipywidgets->jupyter>=1.0.0->scipy-stack) (3.6.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from ipywidgets->jupyter>=1.0.0->scipy-stack) (0.2.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from ipywidgets->jupyter>=1.0.0->scipy-stack) (1.1.1)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from notebook->jupyter>=1.0.0->scipy-stack) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from notebook->jupyter>=1.0.0->scipy-stack) (0.15.0)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from notebook->jupyter>=1.0.0->scipy-stack) (3.1.2)\n",
            "Requirement already satisfied: nbformat in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from notebook->jupyter>=1.0.0->scipy-stack) (5.4.0)\n",
            "Requirement already satisfied: prometheus-client in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from notebook->jupyter>=1.0.0->scipy-stack) (0.14.1)Note: you may need to restart the kernel to use updated packages.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: argon2-cffi in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from notebook->jupyter>=1.0.0->scipy-stack) (21.3.0)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from nbconvert->jupyter>=1.0.0->scipy-stack) (0.6.5)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from nbconvert->jupyter>=1.0.0->scipy-stack) (4.11.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from nbconvert->jupyter>=1.0.0->scipy-stack) (0.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from nbconvert->jupyter>=1.0.0->scipy-stack) (2.1.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from nbconvert->jupyter>=1.0.0->scipy-stack) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from nbconvert->jupyter>=1.0.0->scipy-stack) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from nbconvert->jupyter>=1.0.0->scipy-stack) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from nbconvert->jupyter>=1.0.0->scipy-stack) (1.1.1)\n",
            "Requirement already satisfied: bleach in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from nbconvert->jupyter>=1.0.0->scipy-stack) (5.0.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from nbformat->notebook->jupyter>=1.0.0->scipy-stack) (4.6.1)\n",
            "Requirement already satisfied: fastjsonschema in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from nbformat->notebook->jupyter>=1.0.0->scipy-stack) (2.15.3)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from jsonschema>=2.6->nbformat->notebook->jupyter>=1.0.0->scipy-stack) (0.18.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from jsonschema>=2.6->nbformat->notebook->jupyter>=1.0.0->scipy-stack) (21.4.0)\n",
            "Requirement already satisfied: pywinpty>=1.1.0 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from terminado>=0.8.3->notebook->jupyter>=1.0.0->scipy-stack) (2.0.5)\n",
            "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from argon2-cffi->notebook->jupyter>=1.0.0->scipy-stack) (21.2.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter>=1.0.0->scipy-stack) (1.15.0)\n",
            "Requirement already satisfied: pycparser in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter>=1.0.0->scipy-stack) (2.21)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from beautifulsoup4->nbconvert->jupyter>=1.0.0->scipy-stack) (2.3.2.post1)\n",
            "Requirement already satisfied: webencodings in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from bleach->nbconvert->jupyter>=1.0.0->scipy-stack) (0.5.1)\n",
            "Requirement already satisfied: qtpy>=2.0.1 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from qtconsole->jupyter>=1.0.0->scipy-stack) (2.1.0)\n",
            "Requirement already satisfied: pure-eval in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->scipy-stack) (0.2.2)\n",
            "Requirement already satisfied: executing in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->scipy-stack) (0.8.3)\n",
            "Requirement already satisfied: asttokens in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->scipy-stack) (2.0.5)\n"
          ]
        }
      ],
      "source": [
        "pip install scipy-stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mlxtend in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (0.21.0)Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Requirement already satisfied: scipy>=1.2.1 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from mlxtend) (1.10.0)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from mlxtend) (3.5.2)\n",
            "Requirement already satisfied: setuptools in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from mlxtend) (61.2.0)\n",
            "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from mlxtend) (1.23.0)\n",
            "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from mlxtend) (1.1.0)\n",
            "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from mlxtend) (1.4.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from mlxtend) (1.1.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.4.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (4.25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.11.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (9.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (21.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\labdata\\.conda\\envs\\gpr\\lib\\site-packages (from scikit-learn>=1.0.2->mlxtend) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "pip install mlxtend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ybXTYnWTzOta"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import linalg\n",
        "# seed = 232\n",
        "# np.random.seed(seed)\n",
        "input_path = \"penelitian_tbc_3/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jIqt7nNezlNb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlO8sNxrEa0j",
        "outputId": "60c50029-b61f-4398-8bb1-fe0cd5a6834d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Set: train, normal images: 196, tbc images: 196\n",
            "Set: val, normal images: 65, tbc images: 65\n",
            "Set: test, normal images: 65, tbc images: 65\n"
          ]
        }
      ],
      "source": [
        "#Display total images avaible in train, val dan test sets\n",
        "for _set in ['train','val','test']:\n",
        "  n_normal = len(os.listdir(input_path +_set + '/normal'))\n",
        "  n_infect = len(os.listdir(input_path +_set + '/tbc'))\n",
        "  print('Set: {}, normal images: {}, tbc images: {}'.format(_set, n_normal, n_infect))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bm9qu5amLR_V"
      },
      "outputs": [],
      "source": [
        "#define some constants\n",
        "img_dims = 224\n",
        "batch_size = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eas28xgeZFfH",
        "outputId": "048ae216-9982-411c-f9fd-b42c0a5457bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 392 images belonging to 2 classes.\n",
            "Found 130 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# image augmentations\n",
        "image_gen = ImageDataGenerator(rescale=1./255\n",
        "                               #validation_split=0.2\n",
        "                               )\n",
        "\n",
        "# flow_from_directory generators\n",
        "train_generator = image_gen.flow_from_directory(\n",
        "                  directory = input_path+'train',\n",
        "                  target_size=(img_dims, img_dims),\n",
        "                  class_mode=\"binary\",\n",
        "                  batch_size=batch_size,\n",
        "                  #subset='training'\n",
        "                  )\n",
        "\n",
        "validation_generator = image_gen.flow_from_directory(\n",
        "                  directory = input_path+'val',\n",
        "                  target_size=(img_dims, img_dims),\n",
        "                  class_mode=\"binary\",\n",
        "                  batch_size=batch_size,\n",
        "                  #subset='validation'\n",
        "                  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRM0eAz5sTur",
        "outputId": "dc6a17e4-56cd-4aeb-eab4-ebf079e2be5a"
      },
      "outputs": [],
      "source": [
        "import efficientnet.keras as efn\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "# from tensorflow.keras.metrics import Recall, Precision, Sensitivity, Specificity\n",
        "\n",
        "# Callbacks\n",
        "## Keep the best model\n",
        "# mc = ModelCheckpoint('model.hdf5', \n",
        "                    #  save_best_only=True, \n",
        "                    #  verbose=0, \n",
        "                    #  monitor='val_loss', \n",
        "                    #  mode='min')\n",
        "\n",
        "## Reduce learning rate if it gets stuck in a plateau\n",
        "# rlr = ReduceLROnPlateau(monitor='val_loss', \n",
        "#                         factor=0.3, \n",
        "#                         patience=3, \n",
        "#                         min_lr=0.000001, \n",
        "#                         verbose=1)\n",
        "\n",
        "# Model\n",
        "## Define the base model with EfficientNet weights\n",
        "model = tf.keras.applications.ResNet101(weights = 'imagenet', \n",
        "                           include_top = False, \n",
        "                           input_shape = (img_dims, img_dims, 3),\n",
        "                           pooling='avg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bMh59NKPtKag"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "## Output layer\n",
        "x = model.output\n",
        "#x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(64, activation=\"relu\")(x)\n",
        "x = Dense(32, activation=\"relu\")(x)\n",
        "predictions = Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "## Compile and run\n",
        "model = Model(inputs=model.input, outputs=predictions)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9,name=\"SGD\"),\n",
        "              loss='binary_crossentropy', \n",
        "              #metrics=['accuracy', Precision()])\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8vTeLLVtR_p",
        "outputId": "58a5c5ba-5db4-46c6-c1f2-996dbfc73077"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "98/98 [==============================] - 25s 138ms/step - loss: 0.3285 - accuracy: 0.8444 - val_loss: 0.9489 - val_accuracy: 0.5000\n",
            "Epoch 2/200\n",
            "98/98 [==============================] - 12s 119ms/step - loss: 0.1225 - accuracy: 0.9464 - val_loss: 1.3525 - val_accuracy: 0.5000\n",
            "Epoch 3/200\n",
            "98/98 [==============================] - 12s 119ms/step - loss: 0.0606 - accuracy: 0.9745 - val_loss: 0.7403 - val_accuracy: 0.5462\n",
            "Epoch 4/200\n",
            "98/98 [==============================] - 12s 120ms/step - loss: 0.0363 - accuracy: 0.9872 - val_loss: 0.7122 - val_accuracy: 0.6077\n",
            "Epoch 5/200\n",
            "98/98 [==============================] - 12s 120ms/step - loss: 0.0336 - accuracy: 0.9847 - val_loss: 1.4394 - val_accuracy: 0.4923\n",
            "Epoch 6/200\n",
            "98/98 [==============================] - 12s 120ms/step - loss: 0.0134 - accuracy: 0.9949 - val_loss: 0.9142 - val_accuracy: 0.4538\n",
            "Epoch 7/200\n",
            "98/98 [==============================] - 12s 120ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.8740 - val_accuracy: 0.5077\n",
            "Epoch 8/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 1.3666 - val_accuracy: 0.5077\n",
            "Epoch 9/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.8988 - val_accuracy: 0.6000\n",
            "Epoch 10/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.2071 - val_accuracy: 0.9231\n",
            "Epoch 11/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0627 - val_accuracy: 0.9846\n",
            "Epoch 12/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0322 - val_accuracy: 0.9846\n",
            "Epoch 13/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0247 - val_accuracy: 0.9923\n",
            "Epoch 14/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0181 - val_accuracy: 0.9923\n",
            "Epoch 15/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 8.8994e-04 - accuracy: 1.0000 - val_loss: 0.0089 - val_accuracy: 1.0000\n",
            "Epoch 16/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 9.6625e-04 - accuracy: 1.0000 - val_loss: 0.0142 - val_accuracy: 1.0000\n",
            "Epoch 17/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0363 - val_accuracy: 0.9923\n",
            "Epoch 18/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 6.7085e-04 - accuracy: 1.0000 - val_loss: 0.0265 - val_accuracy: 0.9923\n",
            "Epoch 19/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 2.8142e-04 - accuracy: 1.0000 - val_loss: 0.0217 - val_accuracy: 0.9923\n",
            "Epoch 20/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 4.8342e-04 - accuracy: 1.0000 - val_loss: 0.0219 - val_accuracy: 0.9923\n",
            "Epoch 21/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 4.2450e-04 - accuracy: 1.0000 - val_loss: 0.0174 - val_accuracy: 0.9923\n",
            "Epoch 22/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 4.5328e-04 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9846\n",
            "Epoch 23/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 2.5987e-04 - accuracy: 1.0000 - val_loss: 0.0190 - val_accuracy: 0.9923\n",
            "Epoch 24/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 3.7386e-04 - accuracy: 1.0000 - val_loss: 0.0182 - val_accuracy: 0.9923\n",
            "Epoch 25/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 2.8443e-04 - accuracy: 1.0000 - val_loss: 0.0165 - val_accuracy: 0.9923\n",
            "Epoch 26/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 2.7720e-04 - accuracy: 1.0000 - val_loss: 0.0179 - val_accuracy: 0.9923\n",
            "Epoch 27/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 2.9453e-04 - accuracy: 1.0000 - val_loss: 0.0156 - val_accuracy: 0.9923\n",
            "Epoch 28/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 3.9464e-04 - accuracy: 1.0000 - val_loss: 0.0145 - val_accuracy: 1.0000\n",
            "Epoch 29/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 3.1595e-04 - accuracy: 1.0000 - val_loss: 0.0130 - val_accuracy: 1.0000\n",
            "Epoch 30/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 1.5024e-04 - accuracy: 1.0000 - val_loss: 0.0126 - val_accuracy: 1.0000\n",
            "Epoch 31/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0203 - val_accuracy: 0.9923\n",
            "Epoch 32/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 8.6584e-04 - accuracy: 1.0000 - val_loss: 0.0138 - val_accuracy: 0.9923\n",
            "Epoch 33/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 1.2111e-04 - accuracy: 1.0000 - val_loss: 0.0122 - val_accuracy: 1.0000\n",
            "Epoch 34/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 2.2894e-04 - accuracy: 1.0000 - val_loss: 0.0138 - val_accuracy: 1.0000\n",
            "Epoch 35/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 2.1165e-04 - accuracy: 1.0000 - val_loss: 0.0138 - val_accuracy: 1.0000\n",
            "Epoch 36/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.1719 - val_accuracy: 0.9462\n",
            "Epoch 37/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 0.0062 - accuracy: 0.9974 - val_loss: 0.3441 - val_accuracy: 0.9308\n",
            "Epoch 38/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 9.1003e-04 - accuracy: 1.0000 - val_loss: 0.0424 - val_accuracy: 0.9846\n",
            "Epoch 39/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 3.9466e-04 - accuracy: 1.0000 - val_loss: 0.0243 - val_accuracy: 0.9923\n",
            "Epoch 40/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 3.8359e-04 - accuracy: 1.0000 - val_loss: 0.0145 - val_accuracy: 0.9923\n",
            "Epoch 41/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 3.1327e-04 - accuracy: 1.0000 - val_loss: 0.0135 - val_accuracy: 0.9923\n",
            "Epoch 42/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 2.3498e-04 - accuracy: 1.0000 - val_loss: 0.0124 - val_accuracy: 0.9923\n",
            "Epoch 43/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 3.6643e-04 - accuracy: 1.0000 - val_loss: 0.0122 - val_accuracy: 0.9923\n",
            "Epoch 44/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0439 - val_accuracy: 0.9846\n",
            "Epoch 45/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 4.4485e-04 - accuracy: 1.0000 - val_loss: 0.0188 - val_accuracy: 1.0000\n",
            "Epoch 46/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 2.5870e-04 - accuracy: 1.0000 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
            "Epoch 47/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 3.3474e-04 - accuracy: 1.0000 - val_loss: 0.0107 - val_accuracy: 1.0000\n",
            "Epoch 48/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 1.8245e-04 - accuracy: 1.0000 - val_loss: 0.0102 - val_accuracy: 1.0000\n",
            "Epoch 49/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 4.8676e-04 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
            "Epoch 50/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 7.9812e-04 - accuracy: 1.0000 - val_loss: 0.0174 - val_accuracy: 0.9923\n",
            "Epoch 51/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 4.3650e-04 - accuracy: 1.0000 - val_loss: 0.0074 - val_accuracy: 1.0000\n",
            "Epoch 52/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 1.4453e-04 - accuracy: 1.0000 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
            "Epoch 53/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 3.3255e-04 - accuracy: 1.0000 - val_loss: 0.0058 - val_accuracy: 1.0000\n",
            "Epoch 54/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 3.5017e-04 - accuracy: 1.0000 - val_loss: 0.0128 - val_accuracy: 1.0000\n",
            "Epoch 55/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 1.8921e-04 - accuracy: 1.0000 - val_loss: 0.0084 - val_accuracy: 1.0000\n",
            "Epoch 56/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 3.3348e-04 - accuracy: 1.0000 - val_loss: 0.0065 - val_accuracy: 1.0000\n",
            "Epoch 57/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 1.4035e-04 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 1.0000\n",
            "Epoch 58/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 1.0911e-04 - accuracy: 1.0000 - val_loss: 0.0060 - val_accuracy: 1.0000\n",
            "Epoch 59/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 7.4412e-04 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9692\n",
            "Epoch 60/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 3.1134e-04 - accuracy: 1.0000 - val_loss: 0.0578 - val_accuracy: 0.9769\n",
            "Epoch 61/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 2.6115e-04 - accuracy: 1.0000 - val_loss: 0.0282 - val_accuracy: 0.9923\n",
            "Epoch 62/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 9.4318e-05 - accuracy: 1.0000 - val_loss: 0.0266 - val_accuracy: 0.9923\n",
            "Epoch 63/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0155 - val_accuracy: 0.9923\n",
            "Epoch 64/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 1.2564e-04 - accuracy: 1.0000 - val_loss: 0.0122 - val_accuracy: 0.9923\n",
            "Epoch 65/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 3.0927e-04 - accuracy: 1.0000 - val_loss: 0.0150 - val_accuracy: 0.9923\n",
            "Epoch 66/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 2.0274e-04 - accuracy: 1.0000 - val_loss: 0.0263 - val_accuracy: 0.9923\n",
            "Epoch 67/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 6.2852e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9923\n",
            "Epoch 68/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 7.8227e-05 - accuracy: 1.0000 - val_loss: 0.0202 - val_accuracy: 0.9923\n",
            "Epoch 69/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 1.3962e-04 - accuracy: 1.0000 - val_loss: 0.0171 - val_accuracy: 0.9923\n",
            "Epoch 70/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 4.9915e-05 - accuracy: 1.0000 - val_loss: 0.0156 - val_accuracy: 0.9923\n",
            "Epoch 71/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 7.3301e-05 - accuracy: 1.0000 - val_loss: 0.0156 - val_accuracy: 0.9923\n",
            "Epoch 72/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 5.0484e-05 - accuracy: 1.0000 - val_loss: 0.0149 - val_accuracy: 0.9923\n",
            "Epoch 73/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 6.8233e-05 - accuracy: 1.0000 - val_loss: 0.0151 - val_accuracy: 0.9923\n",
            "Epoch 74/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 4.5147e-05 - accuracy: 1.0000 - val_loss: 0.0161 - val_accuracy: 0.9923\n",
            "Epoch 75/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 5.8959e-05 - accuracy: 1.0000 - val_loss: 0.0158 - val_accuracy: 0.9923\n",
            "Epoch 76/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 1.0791e-04 - accuracy: 1.0000 - val_loss: 0.0155 - val_accuracy: 0.9923\n",
            "Epoch 77/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 4.3176e-05 - accuracy: 1.0000 - val_loss: 0.0149 - val_accuracy: 0.9923\n",
            "Epoch 78/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 3.6718e-05 - accuracy: 1.0000 - val_loss: 0.0150 - val_accuracy: 0.9923\n",
            "Epoch 79/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 1.0867e-04 - accuracy: 1.0000 - val_loss: 0.0121 - val_accuracy: 0.9923\n",
            "Epoch 80/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 1.4429e-04 - accuracy: 1.0000 - val_loss: 0.0149 - val_accuracy: 0.9923\n",
            "Epoch 81/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 4.7710e-05 - accuracy: 1.0000 - val_loss: 0.0161 - val_accuracy: 0.9923\n",
            "Epoch 82/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 4.1252e-05 - accuracy: 1.0000 - val_loss: 0.0167 - val_accuracy: 0.9923\n",
            "Epoch 83/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 1.1968e-04 - accuracy: 1.0000 - val_loss: 0.0177 - val_accuracy: 0.9923\n",
            "Epoch 84/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 7.5132e-05 - accuracy: 1.0000 - val_loss: 0.0189 - val_accuracy: 0.9923\n",
            "Epoch 85/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 5.5273e-05 - accuracy: 1.0000 - val_loss: 0.0191 - val_accuracy: 0.9923\n",
            "Epoch 86/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 6.2236e-05 - accuracy: 1.0000 - val_loss: 0.0178 - val_accuracy: 0.9923\n",
            "Epoch 87/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 3.8616e-05 - accuracy: 1.0000 - val_loss: 0.0162 - val_accuracy: 0.9923\n",
            "Epoch 88/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 4.4839e-05 - accuracy: 1.0000 - val_loss: 0.0155 - val_accuracy: 0.9923\n",
            "Epoch 89/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 4.4481e-05 - accuracy: 1.0000 - val_loss: 0.0146 - val_accuracy: 0.9923\n",
            "Epoch 90/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 4.4108e-05 - accuracy: 1.0000 - val_loss: 0.0148 - val_accuracy: 0.9923\n",
            "Epoch 91/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 7.0449e-05 - accuracy: 1.0000 - val_loss: 0.0156 - val_accuracy: 0.9923\n",
            "Epoch 92/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 3.9623e-05 - accuracy: 1.0000 - val_loss: 0.0146 - val_accuracy: 0.9923\n",
            "Epoch 93/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 4.5979e-05 - accuracy: 1.0000 - val_loss: 0.0147 - val_accuracy: 0.9923\n",
            "Epoch 94/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 3.9968e-05 - accuracy: 1.0000 - val_loss: 0.0140 - val_accuracy: 0.9923\n",
            "Epoch 95/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 2.4138e-05 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 0.9923\n",
            "Epoch 96/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 4.0805e-05 - accuracy: 1.0000 - val_loss: 0.0126 - val_accuracy: 0.9923\n",
            "Epoch 97/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 3.1379e-05 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 0.9923\n",
            "Epoch 98/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 2.9336e-05 - accuracy: 1.0000 - val_loss: 0.0124 - val_accuracy: 0.9923\n",
            "Epoch 99/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 6.1412e-05 - accuracy: 1.0000 - val_loss: 0.0135 - val_accuracy: 0.9923\n",
            "Epoch 100/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 4.1856e-05 - accuracy: 1.0000 - val_loss: 0.0136 - val_accuracy: 0.9923\n",
            "Epoch 101/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 4.2774e-05 - accuracy: 1.0000 - val_loss: 0.0147 - val_accuracy: 0.9923\n",
            "Epoch 102/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 3.6232e-05 - accuracy: 1.0000 - val_loss: 0.0144 - val_accuracy: 0.9923\n",
            "Epoch 103/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 5.2580e-05 - accuracy: 1.0000 - val_loss: 0.0136 - val_accuracy: 0.9923\n",
            "Epoch 104/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 2.9628e-05 - accuracy: 1.0000 - val_loss: 0.0139 - val_accuracy: 0.9923\n",
            "Epoch 105/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 3.4314e-05 - accuracy: 1.0000 - val_loss: 0.0134 - val_accuracy: 0.9923\n",
            "Epoch 106/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 5.8094e-05 - accuracy: 1.0000 - val_loss: 0.0140 - val_accuracy: 0.9923\n",
            "Epoch 107/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 3.2590e-05 - accuracy: 1.0000 - val_loss: 0.0135 - val_accuracy: 0.9923\n",
            "Epoch 108/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 4.7737e-05 - accuracy: 1.0000 - val_loss: 0.0136 - val_accuracy: 0.9923\n",
            "Epoch 109/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 3.9941e-05 - accuracy: 1.0000 - val_loss: 0.0137 - val_accuracy: 0.9923\n",
            "Epoch 110/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 5.2810e-05 - accuracy: 1.0000 - val_loss: 0.0147 - val_accuracy: 0.9923\n",
            "Epoch 111/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 3.1070e-05 - accuracy: 1.0000 - val_loss: 0.0138 - val_accuracy: 0.9923\n",
            "Epoch 112/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 3.1876e-05 - accuracy: 1.0000 - val_loss: 0.0131 - val_accuracy: 0.9923\n",
            "Epoch 113/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 5.8436e-05 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 0.9923\n",
            "Epoch 114/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 2.6992e-05 - accuracy: 1.0000 - val_loss: 0.0136 - val_accuracy: 0.9923\n",
            "Epoch 115/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 5.7075e-05 - accuracy: 1.0000 - val_loss: 0.0137 - val_accuracy: 0.9923\n",
            "Epoch 116/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 3.8881e-05 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 0.9923\n",
            "Epoch 117/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 2.2492e-05 - accuracy: 1.0000 - val_loss: 0.0124 - val_accuracy: 0.9923\n",
            "Epoch 118/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 4.3529e-05 - accuracy: 1.0000 - val_loss: 0.0149 - val_accuracy: 0.9923\n",
            "Epoch 119/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 2.5257e-05 - accuracy: 1.0000 - val_loss: 0.0132 - val_accuracy: 0.9923\n",
            "Epoch 120/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 3.8888e-05 - accuracy: 1.0000 - val_loss: 0.0121 - val_accuracy: 0.9923\n",
            "Epoch 121/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 3.5276e-05 - accuracy: 1.0000 - val_loss: 0.0106 - val_accuracy: 0.9923\n",
            "Epoch 122/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 3.6710e-05 - accuracy: 1.0000 - val_loss: 0.0119 - val_accuracy: 0.9923\n",
            "Epoch 123/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 2.7369e-05 - accuracy: 1.0000 - val_loss: 0.0113 - val_accuracy: 0.9923\n",
            "Epoch 124/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 3.1183e-05 - accuracy: 1.0000 - val_loss: 0.0107 - val_accuracy: 0.9923\n",
            "Epoch 125/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 3.9644e-05 - accuracy: 1.0000 - val_loss: 0.0110 - val_accuracy: 0.9923\n",
            "Epoch 126/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 2.6600e-05 - accuracy: 1.0000 - val_loss: 0.0116 - val_accuracy: 0.9923\n",
            "Epoch 127/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 2.4684e-05 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 0.9923\n",
            "Epoch 128/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 3.9674e-05 - accuracy: 1.0000 - val_loss: 0.0105 - val_accuracy: 0.9923\n",
            "Epoch 129/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 1.8093e-05 - accuracy: 1.0000 - val_loss: 0.0107 - val_accuracy: 0.9923\n",
            "Epoch 130/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 2.5302e-05 - accuracy: 1.0000 - val_loss: 0.0114 - val_accuracy: 0.9923\n",
            "Epoch 131/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 4.5944e-05 - accuracy: 1.0000 - val_loss: 0.0120 - val_accuracy: 0.9923\n",
            "Epoch 132/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 2.0304e-05 - accuracy: 1.0000 - val_loss: 0.0114 - val_accuracy: 0.9923\n",
            "Epoch 133/200\n",
            "98/98 [==============================] - 12s 123ms/step - loss: 3.2968e-05 - accuracy: 1.0000 - val_loss: 0.0120 - val_accuracy: 0.9923\n",
            "Epoch 134/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 2.4259e-05 - accuracy: 1.0000 - val_loss: 0.0120 - val_accuracy: 0.9923\n",
            "Epoch 135/200\n",
            "98/98 [==============================] - 13s 131ms/step - loss: 2.4466e-05 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 0.9923\n",
            "Epoch 136/200\n",
            "98/98 [==============================] - 13s 135ms/step - loss: 2.8448e-05 - accuracy: 1.0000 - val_loss: 0.0108 - val_accuracy: 0.9923\n",
            "Epoch 137/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 2.0053e-05 - accuracy: 1.0000 - val_loss: 0.0114 - val_accuracy: 0.9923\n",
            "Epoch 138/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 3.2122e-05 - accuracy: 1.0000 - val_loss: 0.0123 - val_accuracy: 0.9923\n",
            "Epoch 139/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 4.2435e-05 - accuracy: 1.0000 - val_loss: 0.0117 - val_accuracy: 0.9923\n",
            "Epoch 140/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 2.9978e-05 - accuracy: 1.0000 - val_loss: 0.0110 - val_accuracy: 0.9923\n",
            "Epoch 141/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 2.6755e-05 - accuracy: 1.0000 - val_loss: 0.0114 - val_accuracy: 0.9923\n",
            "Epoch 142/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 2.8708e-05 - accuracy: 1.0000 - val_loss: 0.0094 - val_accuracy: 0.9923\n",
            "Epoch 143/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 3.3118e-05 - accuracy: 1.0000 - val_loss: 0.0097 - val_accuracy: 0.9923\n",
            "Epoch 144/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 4.1775e-05 - accuracy: 1.0000 - val_loss: 0.0087 - val_accuracy: 0.9923\n",
            "Epoch 145/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 4.7738e-05 - accuracy: 1.0000 - val_loss: 0.0088 - val_accuracy: 0.9923\n",
            "Epoch 146/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 1.2129e-04 - accuracy: 1.0000 - val_loss: 0.0096 - val_accuracy: 0.9923\n",
            "Epoch 147/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 2.4715e-05 - accuracy: 1.0000 - val_loss: 0.0106 - val_accuracy: 0.9923\n",
            "Epoch 148/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 1.3893e-05 - accuracy: 1.0000 - val_loss: 0.0101 - val_accuracy: 0.9923\n",
            "Epoch 149/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 1.6497e-05 - accuracy: 1.0000 - val_loss: 0.0096 - val_accuracy: 0.9923\n",
            "Epoch 150/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 2.4540e-05 - accuracy: 1.0000 - val_loss: 0.0092 - val_accuracy: 0.9923\n",
            "Epoch 151/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 2.1729e-05 - accuracy: 1.0000 - val_loss: 0.0091 - val_accuracy: 0.9923\n",
            "Epoch 152/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 1.3433e-05 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 0.9923\n",
            "Epoch 153/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 2.3040e-05 - accuracy: 1.0000 - val_loss: 0.0093 - val_accuracy: 0.9923\n",
            "Epoch 154/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 1.4983e-05 - accuracy: 1.0000 - val_loss: 0.0097 - val_accuracy: 0.9923\n",
            "Epoch 155/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 3.9169e-05 - accuracy: 1.0000 - val_loss: 0.0090 - val_accuracy: 0.9923\n",
            "Epoch 156/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 1.7135e-05 - accuracy: 1.0000 - val_loss: 0.0089 - val_accuracy: 0.9923\n",
            "Epoch 157/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 5.6546e-05 - accuracy: 1.0000 - val_loss: 0.0115 - val_accuracy: 0.9923\n",
            "Epoch 158/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 2.6578e-05 - accuracy: 1.0000 - val_loss: 0.0111 - val_accuracy: 0.9923\n",
            "Epoch 159/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 1.6198e-05 - accuracy: 1.0000 - val_loss: 0.0107 - val_accuracy: 0.9923\n",
            "Epoch 160/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 2.4968e-05 - accuracy: 1.0000 - val_loss: 0.0109 - val_accuracy: 0.9923\n",
            "Epoch 161/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 2.0166e-05 - accuracy: 1.0000 - val_loss: 0.0103 - val_accuracy: 0.9923\n",
            "Epoch 162/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 1.9034e-05 - accuracy: 1.0000 - val_loss: 0.0109 - val_accuracy: 0.9923\n",
            "Epoch 163/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 2.7594e-05 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 0.9923\n",
            "Epoch 164/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 1.6594e-05 - accuracy: 1.0000 - val_loss: 0.0118 - val_accuracy: 0.9923\n",
            "Epoch 165/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 2.2712e-05 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 0.9923\n",
            "Epoch 166/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 1.9231e-05 - accuracy: 1.0000 - val_loss: 0.0105 - val_accuracy: 0.9923\n",
            "Epoch 167/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 1.7222e-05 - accuracy: 1.0000 - val_loss: 0.0113 - val_accuracy: 0.9923\n",
            "Epoch 168/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 1.4361e-05 - accuracy: 1.0000 - val_loss: 0.0122 - val_accuracy: 0.9923\n",
            "Epoch 169/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 2.1454e-05 - accuracy: 1.0000 - val_loss: 0.0121 - val_accuracy: 0.9923\n",
            "Epoch 170/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 2.3282e-05 - accuracy: 1.0000 - val_loss: 0.0109 - val_accuracy: 0.9923\n",
            "Epoch 171/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 1.8328e-05 - accuracy: 1.0000 - val_loss: 0.0114 - val_accuracy: 0.9923\n",
            "Epoch 172/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 1.1903e-05 - accuracy: 1.0000 - val_loss: 0.0106 - val_accuracy: 0.9923\n",
            "Epoch 173/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 1.8614e-05 - accuracy: 1.0000 - val_loss: 0.0101 - val_accuracy: 0.9923\n",
            "Epoch 174/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 3.3365e-05 - accuracy: 1.0000 - val_loss: 0.0106 - val_accuracy: 0.9923\n",
            "Epoch 175/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 1.9159e-05 - accuracy: 1.0000 - val_loss: 0.0101 - val_accuracy: 0.9923\n",
            "Epoch 176/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 2.8268e-05 - accuracy: 1.0000 - val_loss: 0.0124 - val_accuracy: 0.9923\n",
            "Epoch 177/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 2.1336e-05 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 0.9923\n",
            "Epoch 178/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 1.4462e-05 - accuracy: 1.0000 - val_loss: 0.0122 - val_accuracy: 0.9923\n",
            "Epoch 179/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 2.6598e-05 - accuracy: 1.0000 - val_loss: 0.0107 - val_accuracy: 0.9923\n",
            "Epoch 180/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 2.2153e-05 - accuracy: 1.0000 - val_loss: 0.0114 - val_accuracy: 0.9923\n",
            "Epoch 181/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 6.6469e-05 - accuracy: 1.0000 - val_loss: 0.0099 - val_accuracy: 0.9923\n",
            "Epoch 182/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 2.0008e-05 - accuracy: 1.0000 - val_loss: 0.0110 - val_accuracy: 0.9923\n",
            "Epoch 183/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 2.3973e-05 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 0.9923\n",
            "Epoch 184/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 1.6320e-05 - accuracy: 1.0000 - val_loss: 0.0123 - val_accuracy: 0.9923\n",
            "Epoch 185/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 4.8919e-05 - accuracy: 1.0000 - val_loss: 0.0089 - val_accuracy: 0.9923\n",
            "Epoch 186/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 1.3154e-05 - accuracy: 1.0000 - val_loss: 0.0086 - val_accuracy: 0.9923\n",
            "Epoch 187/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 1.6227e-05 - accuracy: 1.0000 - val_loss: 0.0095 - val_accuracy: 0.9923\n",
            "Epoch 188/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 1.0173e-05 - accuracy: 1.0000 - val_loss: 0.0094 - val_accuracy: 0.9923\n",
            "Epoch 189/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 1.6894e-05 - accuracy: 1.0000 - val_loss: 0.0078 - val_accuracy: 0.9923\n",
            "Epoch 190/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 2.0514e-05 - accuracy: 1.0000 - val_loss: 0.0087 - val_accuracy: 0.9923\n",
            "Epoch 191/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 1.5731e-05 - accuracy: 1.0000 - val_loss: 0.0092 - val_accuracy: 0.9923\n",
            "Epoch 192/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 1.5562e-05 - accuracy: 1.0000 - val_loss: 0.0087 - val_accuracy: 0.9923\n",
            "Epoch 193/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 2.0059e-05 - accuracy: 1.0000 - val_loss: 0.0091 - val_accuracy: 0.9923\n",
            "Epoch 194/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 3.0302e-05 - accuracy: 1.0000 - val_loss: 0.0094 - val_accuracy: 0.9923\n",
            "Epoch 195/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 1.8042e-05 - accuracy: 1.0000 - val_loss: 0.0087 - val_accuracy: 0.9923\n",
            "Epoch 196/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 1.5761e-05 - accuracy: 1.0000 - val_loss: 0.0094 - val_accuracy: 0.9923\n",
            "Epoch 197/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 2.2913e-05 - accuracy: 1.0000 - val_loss: 0.0082 - val_accuracy: 0.9923\n",
            "Epoch 198/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 1.6822e-05 - accuracy: 1.0000 - val_loss: 0.0080 - val_accuracy: 0.9923\n",
            "Epoch 199/200\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 1.1486e-05 - accuracy: 1.0000 - val_loss: 0.0078 - val_accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 1.7805e-05 - accuracy: 1.0000 - val_loss: 0.0085 - val_accuracy: 0.9923\n"
          ]
        }
      ],
      "source": [
        "model_history = model.fit(train_generator,\n",
        "                            validation_data=validation_generator,\n",
        "                            steps_per_epoch=train_generator.n/batch_size,\n",
        "                            validation_steps=validation_generator.n/batch_size,\n",
        "                            epochs=200,\n",
        "                            verbose=1)\n",
        "                            # callbacks=[mc])\n",
        "                            #, rlr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "kemudian kita convert datalog dari history model dalam bentuk file .csv\n",
        "file ini dapat didownload di gdrive (akan muncul di tab kiri <<<<)\n",
        "'''\n",
        "\n",
        "import pandas as pd  \n",
        "hist_df = pd.DataFrame(model_history.history) \n",
        "\n",
        "# or save to csv: \n",
        "hist_csv_file = 'history.csv'\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "    hist_df.to_csv(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DSZBC41B8X6A"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 104). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\LabData\\AppData\\Local\\Temp\\tmphbvcneea\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\LabData\\AppData\\Local\\Temp\\tmphbvcneea\\assets\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "model yang telah ditrain juga dapat disimpan dalam bentuk format .tflite dan .h5\n",
        "file .tflite dapat kita benam kedalam smartphone\n",
        "sedangkan file .h5 bisa kita gunakan kembali dengan function load model\n",
        "'''\n",
        "\n",
        "# save file to format .hdf5\n",
        "model.save(\"model_resnet101.h5\")\n",
        "\n",
        "# Convert the model to TFLite format\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "# Convert the model\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save to file\n",
        "with open(\"model.tflite\", \"wb\") as f:\n",
        "  f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "xIZtgLe18cGN",
        "outputId": "77a51c26-b1bd-4ffd-88f4-6eba8c7b8089"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot training and validation accuracy by epoch\n",
        "acc = model_history.history['accuracy']\n",
        "val_acc = model_history.history['val_accuracy']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "AKDHxVITH1KL",
        "outputId": "9b3c2881-d5f9-41f4-be7d-f9ed1d66c032"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot training and validation accuracy by epoch\n",
        "loss = model_history.history['loss']\n",
        "val_loss = model_history.history['val_loss']\n",
        "\n",
        "epochs = range(len(loss))\n",
        "\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.figure()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6_Cso4cIHqI",
        "outputId": "4766247d-4f00-485e-cfd8-13220cada435"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 130 images belonging to 2 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\LabData\\AppData\\Local\\Temp\\ipykernel_2072\\3648189458.py:14: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  preds = model.predict_generator(generator=test_generator) # get proba predictions\n"
          ]
        }
      ],
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "                                    \n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    directory = input_path+\"test\",\n",
        "    target_size=(img_dims, img_dims),\n",
        "    class_mode=\"binary\",\n",
        "    shuffle=False,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "preds = model.predict_generator(generator=test_generator) # get proba predictions\n",
        "labels = 1*(preds > 0.5) # convert proba to classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 1s 34ms/step - loss: 0.0309 - accuracy: 0.9923\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.0308553297072649, 0.9923076629638672]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(test_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "zuP4NDa3IkKB",
        "outputId": "1ee8a7d6-4be6-4bf1-df94-77affe92bdbc"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATkAAAE9CAYAAABwcBXnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAQpklEQVR4nO3dfbRVdZnA8e8jN1IUX9K05PqCJCiaoYLvlbrMsTQtlzaiNjFUTtZUxvi6aiSbZtKkNZX2hqW2xqLSciTL8W3NMjUMqHwX01RGLgaiRmgWir/54x70ipfrEc9m7/v0/azFumefs+/Zz0H8rr3POfucKKUgSVmtU/cAklQlIycpNSMnKTUjJyk1IycpNSMnKbWuugfoK7rWKzF0eN1jqKF23XHrukdQQ82f/xBLliyJ/m5rVuSGDue1Y95X9xhqqJt/dX7dI6ih9t1z/Gpv83BVUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpddQ+gXhttsB7fmHosY0e9kVLgI2d9j4P23pHJR+7Do088CcDU82dy9U131zyp6nbN1f/DyVM+yYoVK5g0+UOccurpdY/UaJVGLiIOAb4CDAG+XUo5u8rtDWbTTj2Ka355N8ee8h1e0zWEYesO5aC9d+S8S/6XL//X9XWPp4ZYsWIFJ33iY/zsqmsZ0d3NfntN4LDDDmfHsWPrHq2xKjtcjYghwNeAdwJjgYkR4X+Jfgxff132220UF18+C4Bnnl3B0iefrnkqNdGc2bMZNepNjNxuO4YOHcrRf38MV/70irrHarQqn5PbA7i/lPJAKWU58APgiAq3N2iNHLEpS554kulnHc+sGafx9TOPZdi6QwH4yDFvY/YPz+CbU49j4+Hr1Typ6rZwYQ/d3Vs9vzxiRDc9PT01TtR8VUZuBPBwn+UFreu0iq6uIYzbYSsuuPRG9p54Dn9++q+cPPkdXHDpjYx992fZ85iz+cOSP3H2lCPrHlU1K6W85LqIqGGSwaPKyPX3N/+S/0IRcUJEzI2IueXZv81DtJ5FT9Cz+I/MuXM+AJdfdyvjdtiKxY8v47nnCqUULvzJzYzfeZuaJ1XdRozoZsGCF/YdenoWsOWWW9Y4UfNVGbkFwFZ9lruBhauuVEqZXkoZX0oZH11/m4djix5bxoI/PMH222wOwP57jGHeA3/gDZtt+Pw6Rxz4Fu7+/SN1jaiGGD9hAvfffx8PPfggy5cv59If/oBDDzu87rEarcpXV+cA20fESKAHOAY4tsLtDWpTzrmUi/5jEkO7hvBQzxJOmHoJXzr1aHYZ000phfmPPM7HPz+j7jFVs66uLv7zK+fz7kP/jhUrVvCBSZMZu9NOdY/VaNHfMX7H7jziXcCX6X0LyYWllH8faP11hm1eXjvmfZXNo8HtiTnn1z2CGmrfPcfz61/P7ffJyUrfJ1dK+Tnw8yq3IUkD8bQuSakZOUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpGTlJqRk5SakZOUmpda3uhohYBpSVi62fpXW5lFI2rHg2SXrVVhu5UsrwtTmIJFWhrcPViNgvIv6xdXmziBhZ7ViS1BkvG7mImAqcBpzRumoocEmVQ0lSp7SzJ/de4HDgKYBSykLAQ1lJg0I7kVteSim0XoSIiPWrHUmSOqedyP0oIr4FbBwRHwauAy6odixJ6ozVvrq6UillWkS8A/gTMBo4s5RybeWTSVIHvGzkWu4A1qP3kPWO6saRpM5q59XVDwGzgSOBo4BbImJy1YNJUie0syd3CrBrKeUxgIjYFPglcGGVg0lSJ7TzwsMCYFmf5WXAw9WMI0mdNdC5q1NaF3uAX0XEFfQ+J3cEvYevktR4Ax2urnzD7+9bf1a6orpxJKmzBjpB/6y1OYgkVeFlX3iIiNcDpwI7AeuuvL6UcmCFc0lSR7TzwsP3gHnASOAs4CFgToUzSVLHtBO5TUsp3wGeKaXcUEqZDOxV8VyS1BHtvE/umdbPRyLiUGAh0F3dSJLUOe1E7vMRsRHwL8B5wIbApyqdSpI6pJ0T9K9sXVwKHFDtOJLUWQO9Gfg8Xvgim5copXyikokkqYMG2pObu9amaBm349bcOOu8tb1ZDRKb7HVS3SOoof46b/Vnmg70ZuDvVjKNJK1Ffrm0pNSMnKTUjJyk1Nr5ZODREXF9RNzZWt4lIj5T/WiS9Oq1syd3Ab1fLP0MQCnlduCYKoeSpE5pJ3LDSimrfkjms1UMI0md1k7klkTEKF74cumjgEcqnUqSOqSdc1c/BkwHdoiIHuBB4PhKp5KkDmnn3NUHgIMiYn1gnVLKspf7HUlqinY+GfjMVZYBKKV8rqKZJKlj2jlcfarP5XWBw4B7qhlHkjqrncPVL/VdjohpwMzKJpKkDlqTMx6GAdt1ehBJqkI7z8ndwQufKzcEeD3g83GSBoV2npM7rM/lZ4FFpRTfDCxpUBgwchGxDvCzUsrOa2keSeqoAZ+TK6U8B9wWEVuvpXkkqaPaOVx9I3BXRMymz9tJSimHVzaVJHVIO5E7q/IpJKki7UTuXaWU0/peERHnADdUM5IkdU4775N7Rz/XvbPTg0hSFQb63tUTgY8C20XE7X1uGg7cXPVgktQJAx2ufh+4CvgCcHqf65eVUh6vdCpJ6pCBvnd1KbAUmLj2xpGkzvLbuiSlZuQkpWbkJKVm5CSlZuQkpWbkJKVm5CSlZuQkpWbkJKVm5CSlZuQkpWbkJKVm5CSlZuQkpWbkJKVm5CSlZuQkpWbkJKVm5CSlZuQkpWbkJKVm5CSlZuQkpWbkJKVm5CSlZuQkpWbkJKVm5CSlZuQkpWbkJKVm5CSlZuQkpWbkJKVm5CSlZuQkpWbkJKVm5CSlZuQa5sQTJrNt9xZM2PXNdY+iBtlog/X4/jmTuPWyM/jtpWew55u3ff62k44/gKfnfplNN1q/vgEbrLLIRcSFEbE4Iu6sahsZHff+Sfz3T6+qeww1zLST38s1v5zHuKO+wB4Tv8i8BxcB0L3Fxhy45xj+75HHa56wuarck7sYOKTC+09pv7e+jU02eV3dY6hBhq//WvbbdRQXX3ELAM88u4KlTz4NwBenvIdPf3UmpdQ5YbN1VXXHpZRfRMS2Vd2/9Ldi5IjNWPLHJ5k+9VjePHpLfnvPw5w87XIO2GN7Fi5eyh33Lax7xEbzOTmp4bqGrMO4Md1ccNnN7H3cNP789HI+c8IhnDb5YD73TZ/aeDm1Ry4iToiIuRExd8mSR+seR2qcnsV/pGfxUubcNR+Ay6+/jXE7dLPNlq9j9oxTmTfzTEZsvhGzvncyW2w6vOZpm6eyw9V2lVKmA9MBdtt9vM8sSKtY9NgyFix6gu232Zz75i9m/z1Gc+u8Bbzro19/fp15M89k3/d/iceWPlXjpM1U+56cXmzS+4/lwLfvw32/u5fR223Fdy/6Tt0jqQGmnPsTLvq345k941TeMnoEX7zo2rpHGjSiVPSyTETMAPYHNgMWAVNLKQP+H7vb7uPLjbPmVDKPBr/N9vlU3SOoof56zwyee2pR9Hdbla+uTqzqviWpXR6uSkrNyElKzchJSs3ISUrNyElKzchJSs3ISUrNyElKzchJSs3ISUrNyElKzchJSs3ISUrNyElKzchJSs3ISUrNyElKzchJSs3ISUrNyElKzchJSs3ISUrNyElKzchJSs3ISUrNyElKzchJSs3ISUrNyElKzchJSs3ISUrNyElKzchJSs3ISUrNyElKzchJSs3ISUrNyElKzchJSs3ISUrNyElKzchJSs3ISUrNyElKzchJSs3ISUrNyElKzchJSs3ISUrNyElKzchJSs3ISUrNyElKzchJSs3ISUrNyElKzchJSs3ISUrNyElKzchJSs3ISUrNyElKzchJSi1KKXXP8LyIeBSYX/ccDbIZsKTuIdRI/tt4sW1KKa/v74ZGRU4vFhFzSynj655DzeO/jfZ5uCopNSMnKTUj12zT6x5AjeW/jTb5nJyk1NyTk5SakWugiDgkIu6NiPsj4vS651FzRMSFEbE4Iu6se5bBwsg1TEQMAb4GvBMYC0yMiLH1TqUGuRg4pO4hBhMj1zx7APeXUh4opSwHfgAcUfNMaohSyi+Ax+ueYzAxcs0zAni4z/KC1nWS1oCRa57o5zpfApfWkJFrngXAVn2Wu4GFNc0iDXpGrnnmANtHxMiIGAocA8yseSZp0DJyDVNKeRb4Z+Bq4B7gR6WUu+qdSk0RETOAWcCYiFgQER+se6am84wHSam5JycpNSMnKTUjJyk1IycpNSMnKTUjp8pFxP4RcWXr8uEDfbJKRGwcER9dg218NiJObvf6Vda5OCKOegXb2tZPARk8jJzWWOsTU16RUsrMUsrZA6yyMfCKIyetjpHTS7T2VOZFxHcj4vaIuCwihrVueygizoyIm4CjI+LgiJgVEb+JiEsjYoPWeoe07uMm4Mg+9z0pIs5vXd4iIi6PiNtaf/YBzgZGRcStEXFua71TImJOa5az+tzXp1ufu3cdMKaNx/Xh1v3cFhE/XvmYWg6KiBsj4ncRcVhr/SERcW6fbf/Tq/271dpn5LQ6Y4DppZRdgD/x4r2rv5RS9gOuAz4DHFRK2Q2YC0yJiHWBC4B3A28F3rCabXwVuKGU8hZgN+Au4HTg96WUcaWUUyLiYGB7ej+Cahywe0S8LSJ2p/eUt13pjeiENh7TT0opE1rbuwfoe7bAtsDbgUOBb7YewweBpaWUCa37/3BEjGxjO2qQrroHUGM9XEq5uXX5EuATwLTW8g9bP/ei94M9b44IgKH0nnK0A/BgKeU+gIi4BDihn20cCPwDQCllBbA0IjZZZZ2DW39+21regN7oDQcuL6X8ubWNds7v3TkiPk/vIfEG9J46t9KPSinPAfdFxAOtx3AwsEuf5+s2am37d21sSw1h5LQ6q57v13f5qdbPAK4tpUzsu2JEjOvn99dUAF8opXxrlW2ctAbbuBh4TynltoiYBOzf57b+Hm8AHy+l9I0hEbHtK9yuauThqlZn64jYu3V5InBTP+vcAuwbEW8CiIhhETEamAeMjIhRfX6/P9cDJ7Z+d0hEbAgso3cvbaWrgcl9nusbERGbA78A3hsR60XEcHoPjV/OcOCRiHgNcNwqtx0dEeu0Zt4OuLe17RNb6xMRoyNi/Ta2owYxclqde4APRMTtwOuAb6y6QinlUWASMKO13i3ADqWUv9B7ePqz1gsP81ezjU8CB0TEHcCvgZ1KKY/Re/h7Z0ScW0q5Bvg+MKu13mXA8FLKb+g9bL4V+DFwYxuP6V+BXwHX0hvivu4FbgCuAj7SegzfBu4GftN6y8i38Ohn0PFTSPQSrcOxK0spO9c9i/RquScnKTX35CSl5p6cpNSMnKTUjJyk1IycpNSMnKTUjJyk1P4ftdx8++YRmHoAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "CM = confusion_matrix(test_generator.classes, labels)\n",
        "fig, ax = plot_confusion_matrix(conf_mat=CM ,  figsize=(5, 5))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.9923076923076923\n",
            "sensitivity: 0.9846153846153847\n",
            "specificity: 1.0\n",
            "precision: 1.0\n"
          ]
        }
      ],
      "source": [
        "tn, fp, fn, tp = CM.ravel()\n",
        "akurasi = (tp + tn) / (tp + tn + fp + fn)\n",
        "sensitivity = tp / (tp + fn)\n",
        "specificity = tn / (tn + fp)\n",
        "precision = tp / (tp + fp)\n",
        "print('accuracy:', akurasi)\n",
        "print('sensitivity:', sensitivity)\n",
        "print('specificity:', specificity)\n",
        "print('precision:', precision)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "55399b9152f53381a05f2fc335a781028082887c300d98c056479698817ac7ac"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
